{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "53f85965",
      "metadata": {
        "id": "53f85965"
      },
      "source": [
        "# SigLIP Ablation Notebook (Standalone + Verbose)\n",
        "This notebook is **standalone**: a **2×2 ablation**:\n",
        "\n",
        "- **Pretrained baseline (no training)**  \n",
        "- **Baseline (trained, no MAN, no HNM)**  \n",
        "- **MAN only** (MAN + standard contrastive loss)  \n",
        "- **HNM only** (HNM + no MAN)  \n",
        "- **MAN + HNM** (both)\n",
        "\n",
        "After each run it computes retrieval metrics:\n",
        "- Recall@1/5/10 for **Image→Text** and **Text→Image**\n",
        "- Mean/Median rank\n",
        "\n",
        "It also produces plots and qualitative retrieval examples.\n",
        "\n",
        " **Verbose**: long steps print progress (dataset saving, epochs, evaluation, qualitative).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "535a8340",
      "metadata": {
        "id": "535a8340"
      },
      "outputs": [],
      "source": [
        "# ===== Install (Colab) =====\n",
        "!pip -q install -U transformers datasets accelerate\n",
        "!pip -q install \"pillow<12\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff7cddf",
      "metadata": {
        "id": "2ff7cddf"
      },
      "outputs": [],
      "source": [
        "# ===== Standard library =====\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "# ===== Third-party =====\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ===== PyTorch =====\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ===== Hugging Face / Accelerate =====\n",
        "from datasets import load_dataset\n",
        "from transformers import SiglipModel, SiglipProcessor\n",
        "from accelerate import Accelerator\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66664a65",
      "metadata": {
        "id": "66664a65"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ===== Global config =====\n",
        "SEED = 42\n",
        "\n",
        "MODEL_NAME = \"google/siglip-base-patch16-224\"\n",
        "\n",
        "# Dataset build size (start smaller for a quick dry-run if needed)\n",
        "TRAIN_N = 25000\n",
        "VAL_N   = 1000\n",
        "\n",
        "# Training hyperparams\n",
        "BATCH_SIZE  = 32\n",
        "MAX_LEN     = 64\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "EPOCHS_DEFAULT = 2\n",
        "LR_DEFAULT = 3e-6\n",
        "WEIGHT_DECAY_DEFAULT = 0.01\n",
        "GRAD_CLIP_DEFAULT = 1.0\n",
        "\n",
        "# Partial unfreezing\n",
        "UNFREEZE_LAST_VISION_DEFAULT = 2\n",
        "UNFREEZE_LAST_TEXT_DEFAULT   = 2\n",
        "\n",
        "# HNM\n",
        "HNM_K_DEFAULT = 10\n",
        "\n",
        "# Paths (Colab/local runtime)\n",
        "BASE_DIR  = \"/content/dataset\"\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
        "VAL_DIR   = os.path.join(BASE_DIR, \"val\")\n",
        "\n",
        "RUNS_DIR    = \"/content/ablation_runs\"\n",
        "RESULTS_DIR = \"/content/ablation_results\"\n",
        "\n",
        "for p in [TRAIN_DIR, VAL_DIR, RUNS_DIR, RESULTS_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "print(\"CUDA:\", torch.cuda.is_available())\n",
        "print(\"TRAIN_DIR:\", TRAIN_DIR)\n",
        "print(\"VAL_DIR:\", VAL_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c008fbb",
      "metadata": {
        "id": "0c008fbb"
      },
      "source": [
        "## 0) Build local dataset (from `jxie/coco_captions`)\n",
        "Creates:\n",
        "```\n",
        "/content/dataset/train/images/*.jpg\n",
        "/content/dataset/train/captions.jsonl\n",
        "/content/dataset/val/images/*.jpg\n",
        "/content/dataset/val/captions.jsonl\n",
        "```\n",
        "If these already exist with enough rows, it will **skip** rebuilding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326a1c44",
      "metadata": {
        "id": "326a1c44"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def _count_jsonl(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        return 0\n",
        "    n = 0\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for _ in f:\n",
        "            n += 1\n",
        "    return n\n",
        "\n",
        "def build_local_dataset_from_coco_captions(train_n: int, val_n: int, seed: int = 42, verbose_every: int = 500):\n",
        "    # ensure split dirs exist\n",
        "    os.makedirs(TRAIN_DIR, exist_ok=True)\n",
        "    os.makedirs(VAL_DIR, exist_ok=True)\n",
        "\n",
        "    train_ann = os.path.join(TRAIN_DIR, \"captions.jsonl\")\n",
        "    val_ann   = os.path.join(VAL_DIR, \"captions.jsonl\")\n",
        "\n",
        "    n_train = _count_jsonl(train_ann)\n",
        "    n_val   = _count_jsonl(val_ann)\n",
        "\n",
        "    if n_train >= train_n and n_val >= val_n:\n",
        "        print(f\" Dataset already exists. train={n_train}, val={n_val} (requested train={train_n}, val={val_n})\")\n",
        "        return\n",
        "\n",
        "    print(\"Building dataset from jxie/coco_captions (streaming)...\")\n",
        "    os.makedirs(os.path.join(TRAIN_DIR, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(VAL_DIR, \"images\"), exist_ok=True)\n",
        "\n",
        "    def save_split_stream(stream, out_dir, n_samples, offset=0, seed_local=42):\n",
        "        random.seed(seed_local)\n",
        "        img_dir  = os.path.join(out_dir, \"images\")\n",
        "        ann_path = os.path.join(out_dir, \"captions.jsonl\")\n",
        "\n",
        "        if offset > 0:\n",
        "            print(f\" Skipping first {offset} samples in stream...\")\n",
        "            stream = stream.skip(offset)\n",
        "\n",
        "        it = iter(stream)\n",
        "        written = 0\n",
        "        t0 = time.time()\n",
        "\n",
        "        with open(ann_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            while written < n_samples:\n",
        "                ex = next(it)\n",
        "\n",
        "                img = ex.get(\"image\", None)\n",
        "                cap = ex.get(\"caption\", None)\n",
        "\n",
        "                if img is None or not isinstance(cap, str) or not cap.strip():\n",
        "                    continue\n",
        "\n",
        "                img = img.convert(\"RGB\")\n",
        "                img_name = f\"{written:06d}.jpg\"\n",
        "                img.save(os.path.join(img_dir, img_name), quality=95)\n",
        "\n",
        "                f.write(json.dumps({\"image\": img_name, \"text\": cap.strip()}, ensure_ascii=False) + \"\\n\")\n",
        "                written += 1\n",
        "\n",
        "                if written % verbose_every == 0:\n",
        "                    dt = time.time() - t0\n",
        "                    rate = written / max(dt, 1e-6)\n",
        "                    print(f\"  -> {out_dir}: saved {written}/{n_samples}  ({rate:.1f} samples/s)\")\n",
        "\n",
        "        print(f\"Done: {out_dir} ({written} samples)\")\n",
        "\n",
        "    # Train\n",
        "    ds = load_dataset(\"jxie/coco_captions\", split=\"train\", streaming=True)\n",
        "    save_split_stream(ds, TRAIN_DIR, train_n, offset=0, seed_local=seed)\n",
        "\n",
        "    # Val (continue stream by skipping train_n)\n",
        "    ds2 = load_dataset(\"jxie/coco_captions\", split=\"train\", streaming=True)\n",
        "    save_split_stream(ds2, VAL_DIR, val_n, offset=train_n, seed_local=seed + 1)\n",
        "\n",
        "build_local_dataset_from_coco_captions(TRAIN_N, VAL_N, seed=SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ===== Sanity checks for local dataset =====\n",
        "def sanity_check_split(split_dir: str, n_show: int = 2):\n",
        "    ann = os.path.join(split_dir, \"captions.jsonl\")\n",
        "    img_dir = os.path.join(split_dir, \"images\")\n",
        "\n",
        "    assert os.path.exists(ann), f\"Missing {ann}\"\n",
        "    assert os.path.exists(img_dir), f\"Missing {img_dir}\"\n",
        "\n",
        "    rows = []\n",
        "    with open(ann, \"r\", encoding=\"utf-8\") as f:\n",
        "        for _ in range(n_show):\n",
        "            rows.append(json.loads(next(f)))\n",
        "\n",
        "    print(\"Split:\", split_dir)\n",
        "    for r in rows:\n",
        "        img_path = os.path.join(img_dir, r[\"image\"])\n",
        "        assert os.path.exists(img_path), f\"Missing image file: {img_path}\"\n",
        "        im = Image.open(img_path).convert(\"RGB\")\n",
        "        print(\"  row:\", r[\"image\"], \"| text:\", r[\"text\"][:80], \"| size:\", im.size)\n",
        "\n",
        "sanity_check_split(TRAIN_DIR)\n",
        "sanity_check_split(VAL_DIR)\n"
      ],
      "metadata": {
        "id": "7rdEP7rEL2HM"
      },
      "id": "7rdEP7rEL2HM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "abf6a458",
      "metadata": {
        "id": "abf6a458"
      },
      "source": [
        "## 1) DataLoaders (local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40aa2618",
      "metadata": {
        "id": "40aa2618"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class LocalImageTextDataset(Dataset):\n",
        "    def __init__(self, root_dir: str):\n",
        "        self.root_dir = root_dir\n",
        "        self.img_dir = os.path.join(root_dir, \"images\")\n",
        "        ann_path = os.path.join(root_dir, \"captions.jsonl\")\n",
        "\n",
        "        if not os.path.exists(ann_path):\n",
        "            raise FileNotFoundError(f\"Missing annotations file: {ann_path}\")\n",
        "        if not os.path.exists(self.img_dir):\n",
        "            raise FileNotFoundError(f\"Missing images directory: {self.img_dir}\")\n",
        "\n",
        "        self.items = []\n",
        "        with open(ann_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                obj = json.loads(line)\n",
        "                if (\n",
        "                    isinstance(obj, dict)\n",
        "                    and \"image\" in obj\n",
        "                    and \"text\" in obj\n",
        "                    and isinstance(obj[\"text\"], str)\n",
        "                    and obj[\"text\"].strip()\n",
        "                ):\n",
        "                    # Optional: keep only if file exists\n",
        "                    img_path = os.path.join(self.img_dir, obj[\"image\"])\n",
        "                    if os.path.exists(img_path):\n",
        "                        self.items.append(obj)\n",
        "\n",
        "        if len(self.items) == 0:\n",
        "            raise ValueError(f\"No valid samples found in {ann_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.items[idx]\n",
        "        img_path = os.path.join(self.img_dir, item[\"image\"])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        return img, item[\"text\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e607f6b",
      "metadata": {
        "id": "0e607f6b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "processor = SiglipProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, texts = zip(*batch)\n",
        "    return processor(\n",
        "        images=list(images),\n",
        "        text=list(texts),\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "train_ds = LocalImageTextDataset(TRAIN_DIR)\n",
        "val_ds   = LocalImageTextDataset(VAL_DIR)\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_dl = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_ds), \"| Val size:\", len(val_ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb5ce76",
      "metadata": {
        "id": "4fb5ce76"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "ann_path = \"/content/dataset/train/captions.jsonl\"\n",
        "img_dir  = \"/content/dataset/train/images\"\n",
        "\n",
        "print(\"ann_path:\", ann_path)\n",
        "print(\"exists?\", os.path.exists(ann_path))\n",
        "print(\"size bytes:\", os.path.getsize(ann_path))\n",
        "\n",
        "with open(ann_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    first = json.loads(next(f))\n",
        "\n",
        "print(\"first row:\", first)\n",
        "\n",
        "img_name = first[\"image\"]              # <-- במקום image_path\n",
        "img_path = os.path.join(img_dir, img_name)\n",
        "\n",
        "print(\"image exists?\", os.path.exists(img_path), \"| path:\", img_path)\n",
        "\n",
        "im = Image.open(img_path).convert(\"RGB\")\n",
        "print(\"image size:\", im.size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd008485",
      "metadata": {
        "id": "cd008485"
      },
      "outputs": [],
      "source": [
        "import json, os\n",
        "\n",
        "ann = os.path.join(TRAIN_DIR, \"captions.jsonl\")\n",
        "with open(ann, \"r\", encoding=\"utf-8\") as f:\n",
        "    first = json.loads(next(f))\n",
        "print(first)\n",
        "print(\"keys:\", list(first.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f6dbeae",
      "metadata": {
        "id": "5f6dbeae"
      },
      "source": [
        "## 2) MAN head + Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766d0e51",
      "metadata": {
        "id": "766d0e51"
      },
      "outputs": [],
      "source": [
        "class MANHead(nn.Module):\n",
        "    \"\"\"ẑ_img = α * LN_img(z_img), ẑ_txt = α * LN_txt(z_txt)\"\"\"\n",
        "    def __init__(self, embed_dim, init_alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.ln_img = nn.LayerNorm(embed_dim)\n",
        "        self.ln_txt = nn.LayerNorm(embed_dim)\n",
        "        self.alpha  = nn.Parameter(torch.tensor(float(init_alpha)))\n",
        "\n",
        "    def forward(self, z_img, z_txt, l2_normalize=False):\n",
        "        z_img = self.alpha * self.ln_img(z_img)\n",
        "        z_txt = self.alpha * self.ln_txt(z_txt)\n",
        "        if l2_normalize:\n",
        "            z_img = F.normalize(z_img, dim=-1)\n",
        "            z_txt = F.normalize(z_txt, dim=-1)\n",
        "        return z_img, z_txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ef9461",
      "metadata": {
        "id": "f6ef9461"
      },
      "outputs": [],
      "source": [
        "def contrastive_ce_loss(S: torch.Tensor) -> torch.Tensor:\n",
        "    B = S.size(0)\n",
        "    labels = torch.arange(B, device=S.device)\n",
        "    loss_i2t = F.cross_entropy(S, labels)\n",
        "    loss_t2i = F.cross_entropy(S.t(), labels)\n",
        "    return 0.5 * (loss_i2t + loss_t2i)\n",
        "\n",
        "def hnm_one_direction(S: torch.Tensor, k: int = 10, neg_weight: float = 1.0) -> torch.Tensor:\n",
        "    B = S.size(0)\n",
        "\n",
        "    # positive pairs\n",
        "    pos = torch.diag(S)\n",
        "    pos_loss = -F.logsigmoid(pos).mean()\n",
        "\n",
        "    # negatives (mask diagonal)\n",
        "    diag_mask = torch.eye(B, dtype=torch.bool, device=S.device)\n",
        "    neg = S.masked_fill(diag_mask, -1e9)\n",
        "\n",
        "    k_eff = min(k, B - 1)\n",
        "    hard_neg, _ = torch.topk(neg, k=k_eff, dim=1)\n",
        "    neg_loss = -F.logsigmoid(-hard_neg).mean()\n",
        "\n",
        "    return pos_loss + neg_weight * neg_loss\n",
        "\n",
        "\n",
        "def hnm_symmetric(S: torch.Tensor, k: int = 10, neg_weight: float = 1.0) -> torch.Tensor:\n",
        "    return 0.5 * (\n",
        "        hnm_one_direction(S, k, neg_weight) +\n",
        "        hnm_one_direction(S.t(), k, neg_weight)\n",
        "    )\n",
        "\n",
        "\n",
        "def siglip_sigmoid_loss(S):\n",
        "    # positives: diag\n",
        "    pos = -F.logsigmoid(torch.diag(S)).mean()\n",
        "\n",
        "    # negatives: all off-diagonal entries\n",
        "    B = S.size(0)\n",
        "    mask = torch.eye(B, device=S.device, dtype=torch.bool)\n",
        "    neg_vals = S.masked_select(~mask)\n",
        "    neg = -F.logsigmoid(-neg_vals).mean()\n",
        "\n",
        "    return pos + neg\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "107b4191",
      "metadata": {
        "id": "107b4191"
      },
      "source": [
        "## 3) Model builder + partial unfreezing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53f2eca",
      "metadata": {
        "id": "a53f2eca"
      },
      "outputs": [],
      "source": [
        "def infer_embed_dim(model: SiglipModel) -> int:\n",
        "    # Try common config fields\n",
        "    for attr in [\"projection_dim\", \"embed_dim\", \"hidden_size\"]:\n",
        "        if hasattr(model.config, attr):\n",
        "            v = getattr(model.config, attr)\n",
        "            if isinstance(v, int) and v > 0:\n",
        "                return v\n",
        "\n",
        "    # Try nested configs\n",
        "    for path in [\n",
        "        (\"vision_config\", \"hidden_size\"),\n",
        "        (\"text_config\", \"hidden_size\"),\n",
        "    ]:\n",
        "        cfg = getattr(model.config, path[0], None)\n",
        "        if cfg is not None and hasattr(cfg, path[1]):\n",
        "            v = getattr(cfg, path[1])\n",
        "            if isinstance(v, int) and v > 0:\n",
        "                return v\n",
        "\n",
        "    # Fallback: run a tiny forward pass to read embedding dim\n",
        "    dummy = SiglipProcessor.from_pretrained(MODEL_NAME)(\n",
        "        images=[Image.new(\"RGB\", (224, 224))],\n",
        "        text=[\"dummy\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=8,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        out = model(**dummy)\n",
        "    return out.image_embeds.shape[-1]\n",
        "\n",
        "def build_model_and_heads(use_man: bool):\n",
        "    model = SiglipModel.from_pretrained(MODEL_NAME)\n",
        "    embed_dim = infer_embed_dim(model)\n",
        "    man = MANHead(embed_dim, init_alpha=1.0) if use_man else None\n",
        "    return model, man, embed_dim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p2mDCPG43BNg",
      "metadata": {
        "id": "p2mDCPG43BNg"
      },
      "outputs": [],
      "source": [
        "def apply_partial_unfreeze(\n",
        "    model,\n",
        "    unfreeze_last_vision: int = 2,\n",
        "    unfreeze_last_text: int = 2,\n",
        "    unfreeze_logit_scale: bool = True\n",
        "):\n",
        "    \"\"\"\n",
        "    Freeze all parameters, then unfreeze:\n",
        "    - last N vision transformer blocks (if N > 0)\n",
        "    - last N text transformer blocks (if N > 0)\n",
        "    - (optionally) logit_scale\n",
        "\n",
        "    Returns:\n",
        "        trainable_params, total_params\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Freeze everything\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # 2) Unfreeze last N vision blocks (only if N > 0)\n",
        "    if hasattr(model, \"vision_model\") and unfreeze_last_vision > 0:\n",
        "        vision_layers = model.vision_model.encoder.layers\n",
        "        n = min(unfreeze_last_vision, len(vision_layers))\n",
        "        for blk in vision_layers[-n:]:\n",
        "            for p in blk.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "    # 3) Unfreeze last N text blocks (only if N > 0)\n",
        "    if hasattr(model, \"text_model\") and unfreeze_last_text > 0:\n",
        "        text_layers = model.text_model.encoder.layers\n",
        "        n = min(unfreeze_last_text, len(text_layers))\n",
        "        for blk in text_layers[-n:]:\n",
        "            for p in blk.parameters():\n",
        "                p.requires_grad = True\n",
        "\n",
        "    # 4) Unfreeze logit_scale (important for contrastive / sigmoid learning)\n",
        "    if unfreeze_logit_scale and hasattr(model, \"logit_scale\"):\n",
        "        if isinstance(model.logit_scale, torch.nn.Parameter):\n",
        "            model.logit_scale.requires_grad = True\n",
        "\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total     = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    return trainable, total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e6b7c4",
      "metadata": {
        "id": "44e6b7c4"
      },
      "outputs": [],
      "source": [
        "class LogitBias(nn.Module):\n",
        "    \"\"\"Trainable scalar bias added to logits: logits = t * sim + b\"\"\"\n",
        "    def __init__(self, init_bias: float = -10.0):\n",
        "        super().__init__()\n",
        "        self.bias = nn.Parameter(torch.tensor(float(init_bias)))\n",
        "\n",
        "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
        "        return logits + self.bias\n",
        "\n",
        "def build_param_groups(model, man: Optional[MANHead], logit_bias: LogitBias, weight_decay: float):\n",
        "    \"\"\"\n",
        "    Weight-decay policy for fine-tuning:\n",
        "      - 0 decay on pretrained encoders (vision_model.*, text_model.*)\n",
        "      - apply decay only on newly-added heads / scale / MAN (safe)\n",
        "      - logit_bias gets 0 decay (bias)\n",
        "    \"\"\"\n",
        "    decay, no_decay = [], []\n",
        "\n",
        "    def is_encoder(name: str) -> bool:\n",
        "        return name.startswith(\"vision_model.\") or name.startswith(\"text_model.\")\n",
        "\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if is_encoder(name):\n",
        "            no_decay.append(p)\n",
        "        else:\n",
        "            decay.append(p)\n",
        "\n",
        "    if man is not None:\n",
        "        for p in man.parameters():\n",
        "            if p.requires_grad:\n",
        "                decay.append(p)\n",
        "\n",
        "    for p in logit_bias.parameters():\n",
        "        no_decay.append(p)\n",
        "\n",
        "    return [\n",
        "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
        "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5825563",
      "metadata": {
        "id": "f5825563"
      },
      "source": [
        "## 4) Evaluation (device-safe + verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff112b50",
      "metadata": {
        "id": "ff112b50"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_embeddings(model, man, dataloader, accelerator: Accelerator, use_cosine: bool = True, desc: str = \"embed\"):\n",
        "    model.eval()\n",
        "    if man is not None:\n",
        "        man.eval()\n",
        "\n",
        "    all_img, all_txt = [], []\n",
        "    for batch in tqdm(dataloader, leave=False, desc=desc):\n",
        "        batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
        "        out = model(**batch)\n",
        "        zi, zt = out.image_embeds, out.text_embeds\n",
        "\n",
        "        if man is not None:\n",
        "            # Always normalize for retrieval-style evaluation\n",
        "            zi, zt = man(zi, zt, l2_normalize=use_cosine)\n",
        "        else:\n",
        "            if use_cosine:\n",
        "                zi = F.normalize(zi, dim=-1)\n",
        "                zt = F.normalize(zt, dim=-1)\n",
        "\n",
        "        all_img.append(zi.detach().float().cpu())\n",
        "        all_txt.append(zt.detach().float().cpu())\n",
        "    if not accelerator.is_main_process:\n",
        "        return torch.empty(0), torch.empty(0)\n",
        "\n",
        "\n",
        "    img_embeds = torch.cat(all_img, dim=0)\n",
        "    txt_embeds = torch.cat(all_txt, dim=0)\n",
        "    return img_embeds, txt_embeds\n",
        "\n",
        "def _recall_at_k(sorted_idx: torch.Tensor, k: int):\n",
        "    # sorted_idx: [N, N] indices sorted desc\n",
        "    N = sorted_idx.size(0)\n",
        "    hits = (sorted_idx[:, :k] == torch.arange(N).unsqueeze(1)).any(dim=1)\n",
        "    return hits.float().mean().item()\n",
        "\n",
        "def _ranks_stats(sorted_idx: torch.Tensor):\n",
        "    N = sorted_idx.size(0)\n",
        "    target = torch.arange(N)\n",
        "    ranks = torch.empty(N, dtype=torch.long)\n",
        "    for i in range(N):\n",
        "        ranks[i] = (sorted_idx[i] == target[i]).nonzero(as_tuple=True)[0].item() + 1  # 1-based\n",
        "    return ranks.float().mean().item(), ranks.float().median().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_retrieval_metrics(model, man, dataloader, accelerator: Accelerator, use_cosine: bool = True) -> Dict[str, float]:\n",
        "    img_emb, txt_emb = compute_embeddings(model, man, dataloader, accelerator, use_cosine=use_cosine, desc=\"embed(val)\")\n",
        "    S = img_emb @ txt_emb.T  # cosine-like if normalized\n",
        "\n",
        "    S_i2t = torch.argsort(S, dim=1, descending=True)\n",
        "    S_t2i = torch.argsort(S.T, dim=1, descending=True)\n",
        "\n",
        "    metrics = {}\n",
        "    for k in [1, 5, 10]:\n",
        "        metrics[f\"i2t_R@{k}\"] = _recall_at_k(S_i2t, k)\n",
        "        metrics[f\"t2i_R@{k}\"] = _recall_at_k(S_t2i, k)\n",
        "\n",
        "    mean_r_i2t, med_r_i2t = _ranks_stats(S_i2t)\n",
        "    mean_r_t2i, med_r_t2i = _ranks_stats(S_t2i)\n",
        "    metrics[\"i2t_mean_rank\"] = mean_r_i2t\n",
        "    metrics[\"i2t_median_rank\"] = med_r_i2t\n",
        "    metrics[\"t2i_mean_rank\"] = mean_r_t2i\n",
        "    metrics[\"t2i_median_rank\"] = med_r_t2i\n",
        "    metrics[\"N_val\"] = int(S.size(0))\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43430e6a",
      "metadata": {
        "id": "43430e6a"
      },
      "source": [
        "## 5) Experiment runner (verbose training + safe saving + free GPU memory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def _save_checkpoint(exp_dir: str, model, processor, man_head, logit_bias_module, accelerator):\n",
        "    \"\"\"\n",
        "    Minimal checkpoint saver.\n",
        "    Saves:\n",
        "      - HuggingFace model via save_pretrained\n",
        "      - processor via save_pretrained\n",
        "      - MAN head (optional) as man_head.pt\n",
        "      - logit bias (optional) as logit_bias.pt\n",
        "    \"\"\"\n",
        "    if not accelerator.is_main_process:\n",
        "        return\n",
        "\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "    # unwrap model (important when using Accelerator)\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "\n",
        "    # 1) Save model + processor\n",
        "    unwrapped_model.save_pretrained(exp_dir, safe_serialization=False)\n",
        "    processor.save_pretrained(exp_dir)\n",
        "\n",
        "    # 2) Save MAN head if exists\n",
        "    if man_head is not None:\n",
        "        torch.save(man_head.state_dict(), os.path.join(exp_dir, \"man_head.pt\"))\n",
        "\n",
        "    # 3) Save logit bias module if exists (optional but recommended)\n",
        "    if logit_bias_module is not None:\n",
        "        torch.save(logit_bias_module.state_dict(), os.path.join(exp_dir, \"logit_bias.pt\"))\n",
        "\n",
        "    print(f\"Saved checkpoint to: {exp_dir}\")\n"
      ],
      "metadata": {
        "id": "3EKTbeOJS_9s"
      },
      "id": "3EKTbeOJS_9s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "310d6e6f",
      "metadata": {
        "id": "310d6e6f"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class ExpConfig:\n",
        "    name: str\n",
        "    use_man: bool\n",
        "    use_hnm: bool\n",
        "    hnm_k: int = HNM_K_DEFAULT\n",
        "    hnm_warmup_epochs: int = 2\n",
        "    epochs: int = EPOCHS_DEFAULT\n",
        "    lr: float = LR_DEFAULT\n",
        "    weight_decay: float = WEIGHT_DECAY_DEFAULT\n",
        "    grad_clip: float = GRAD_CLIP_DEFAULT\n",
        "    unfreeze_last_vision: int = UNFREEZE_LAST_VISION_DEFAULT\n",
        "    unfreeze_last_text: int = UNFREEZE_LAST_TEXT_DEFAULT\n",
        "    man_l2_train: bool = True\n",
        "    log_every_steps: int = 200\n",
        "\n",
        "MIXED_PRECISION = \"fp16\" if torch.cuda.is_available() else \"no\"\n",
        "\n",
        "def train_one_experiment(cfg: ExpConfig):\n",
        "    set_seed(SEED)\n",
        "    accelerator = Accelerator(mixed_precision=MIXED_PRECISION)\n",
        "\n",
        "    exp_dir = os.path.join(RESULTS_DIR, cfg.name)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "    model, man, embed_dim = build_model_and_heads(cfg.use_man)\n",
        "\n",
        "\n",
        "    logit_bias = LogitBias(init_bias=-10.0)\n",
        "\n",
        "\n",
        "    trainable, total = apply_partial_unfreeze(\n",
        "        model,\n",
        "        cfg.unfreeze_last_vision,\n",
        "        cfg.unfreeze_last_text,\n",
        "        True\n",
        "    )\n",
        "    if accelerator.is_main_process:\n",
        "        print(f\"Params: trainable={trainable:,} / total={total:,} ({100*trainable/total:.2f}%)\")\n",
        "\n",
        "    # Optimizer with SigLIP-friendly weight decay policy\n",
        "    param_groups = build_param_groups(model, man, logit_bias, weight_decay=cfg.weight_decay)\n",
        "    optimizer = torch.optim.AdamW(param_groups, lr=cfg.lr)\n",
        "\n",
        "    if man is None:\n",
        "        model, optimizer, tr_dl, va_dl = accelerator.prepare(model, optimizer, train_dl, val_dl)\n",
        "    else:\n",
        "        model, man, optimizer, tr_dl, va_dl = accelerator.prepare(model, man, optimizer, train_dl, val_dl)\n",
        "\n",
        "    logit_bias = accelerator.prepare(logit_bias)\n",
        "\n",
        "    def compute_logits(out) -> torch.Tensor:\n",
        "        zi, zt = out.image_embeds, out.text_embeds\n",
        "\n",
        "        if man is not None:\n",
        "            zi, zt = man(zi, zt, l2_normalize=cfg.man_l2_train)\n",
        "        else:\n",
        "            zi = F.normalize(zi, dim=-1)\n",
        "            zt = F.normalize(zt, dim=-1)\n",
        "\n",
        "        t = model.logit_scale.exp()\n",
        "        logits = t * (zi @ zt.T)\n",
        "\n",
        "\n",
        "        logits = logit_bias(logits)\n",
        "        return logits\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": []}\n",
        "\n",
        "    for ep in range(cfg.epochs):\n",
        "        model.train()\n",
        "        if man is not None:\n",
        "            man.train()\n",
        "        logit_bias.train()\n",
        "\n",
        "        running = 0.0\n",
        "        steps = 0\n",
        "\n",
        "\n",
        "        if cfg.use_hnm and cfg.hnm_warmup_epochs and cfg.hnm_warmup_epochs > 0:\n",
        "            neg_weight = min(1.0, float(ep + 1) / float(cfg.hnm_warmup_epochs))\n",
        "        else:\n",
        "            neg_weight = 1.0\n",
        "\n",
        "        for step, batch in enumerate(tr_dl):\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            out = model(**batch)\n",
        "            S = compute_logits(out)\n",
        "\n",
        "            if cfg.use_hnm:\n",
        "                loss = hnm_symmetric(S, k=cfg.hnm_k, neg_weight=neg_weight)\n",
        "            else:\n",
        "                loss = siglip_sigmoid_loss(S)\n",
        "\n",
        "            accelerator.backward(loss)\n",
        "\n",
        "            if cfg.grad_clip is not None and cfg.grad_clip > 0:\n",
        "                params_to_clip = list(model.parameters())\n",
        "                if man is not None:\n",
        "                    params_to_clip += list(man.parameters())\n",
        "                params_to_clip += list(logit_bias.parameters())\n",
        "                accelerator.clip_grad_norm_(params_to_clip, cfg.grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "            if accelerator.is_main_process and cfg.log_every_steps and (step + 1) % cfg.log_every_steps == 0:\n",
        "                extra = f\" | neg_w={neg_weight:.2f}\" if cfg.use_hnm else \"\"\n",
        "                print(f\"Epoch {ep+1}/{cfg.epochs} step {step+1}: loss={running/steps:.4f}{extra}\")\n",
        "\n",
        "        train_loss = running / max(1, steps)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        if man is not None:\n",
        "            man.eval()\n",
        "        logit_bias.eval()\n",
        "\n",
        "        vrun = 0.0\n",
        "        vsteps = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in va_dl:\n",
        "                out = model(**batch)\n",
        "                S = compute_logits(out)\n",
        "                if cfg.use_hnm:\n",
        "                    vloss = hnm_symmetric(S, k=cfg.hnm_k, neg_weight=1.0)\n",
        "                else:\n",
        "                    vloss = siglip_sigmoid_loss(S)\n",
        "                vrun += vloss.item()\n",
        "                vsteps += 1\n",
        "\n",
        "        val_loss = vrun / max(1, vsteps)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "\n",
        "        if accelerator.is_main_process:\n",
        "            print(f\"Epoch {ep+1}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "\n",
        "    metrics = compute_retrieval_metrics(model, man, va_dl, accelerator, use_cosine=True)\n",
        "\n",
        "    if accelerator.is_main_process:\n",
        "        _save_checkpoint(\n",
        "            exp_dir=exp_dir,\n",
        "            model=model,\n",
        "            processor=processor,\n",
        "            man_head=man,\n",
        "            logit_bias_module=logit_bias,\n",
        "            accelerator=accelerator\n",
        "        )\n",
        "\n",
        "    result = {\n",
        "        \"config\": asdict(cfg),\n",
        "        \"history\": history,\n",
        "        \"metrics\": metrics,\n",
        "        \"checkpoint_dir\": exp_dir,\n",
        "    }\n",
        "\n",
        "    accelerator.free_memory()\n",
        "    del model\n",
        "    if man is not None:\n",
        "        del man\n",
        "    del logit_bias\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d33988",
      "metadata": {
        "id": "c9d33988"
      },
      "source": [
        "## 6) Pretrained baseline (no training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c79e656e",
      "metadata": {
        "id": "c79e656e"
      },
      "outputs": [],
      "source": [
        "def evaluate_pretrained_baseline() -> Dict:\n",
        "    set_seed(SEED)\n",
        "    accelerator = Accelerator(mixed_precision=\"fp16\" if torch.cuda.is_available() else \"no\")\n",
        "    model = SiglipModel.from_pretrained(MODEL_NAME)\n",
        "    model = accelerator.prepare(model)\n",
        "\n",
        "    metrics = compute_retrieval_metrics(model, None, val_dl, accelerator, use_cosine=True)\n",
        "    if accelerator.is_main_process:\n",
        "        print(\"[pretrained_baseline] Metrics:\", metrics)\n",
        "\n",
        "    exp_dir = os.path.join(RUNS_DIR, \"pretrained_baseline\")\n",
        "    if accelerator.is_main_process:\n",
        "        os.makedirs(exp_dir, exist_ok=True)\n",
        "        accelerator.unwrap_model(model).to(\"cpu\").save_pretrained(exp_dir, safe_serialization=False)\n",
        "        processor.save_pretrained(exp_dir)\n",
        "\n",
        "    result = {\n",
        "        \"config\": {\"name\": \"pretrained_baseline\", \"trained\": False},\n",
        "        \"history\": {\"train_loss\": [], \"val_loss\": []},\n",
        "        \"metrics\": metrics,\n",
        "        \"checkpoint_dir\": exp_dir,\n",
        "    }\n",
        "\n",
        "    accelerator.free_memory()\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a974d65e",
      "metadata": {
        "id": "a974d65e"
      },
      "source": [
        "## 7) Run ablations (verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8f5fe8",
      "metadata": {
        "id": "ab8f5fe8"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "\n",
        "RESULTS_PATH = os.path.join(RESULTS_DIR, \"results.json\")\n",
        "\n",
        "def load_results():\n",
        "    if os.path.exists(RESULTS_PATH):\n",
        "        with open(RESULTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "            r = json.load(f)\n",
        "        print(\" Loaded existing results:\", list(r.keys()))\n",
        "        return r\n",
        "    print(\" No existing results.json found. Starting fresh.\")\n",
        "    return {}\n",
        "\n",
        "def save_results(results):\n",
        "    with open(RESULTS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    print(\" Saved:\", RESULTS_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3hSnSmYifbF",
      "metadata": {
        "id": "b3hSnSmYifbF"
      },
      "outputs": [],
      "source": [
        "results = load_results()\n",
        "\n",
        "if \"pretrained_baseline\" not in results:\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"Running: pretrained_baseline (no training)\")\n",
        "    results[\"pretrained_baseline\"] = evaluate_pretrained_baseline()\n",
        "    save_results(results)\n",
        "else:\n",
        "    print(\"Skipping pretrained_baseline (already saved)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fp7wMt39iiT9",
      "metadata": {
        "id": "Fp7wMt39iiT9"
      },
      "outputs": [],
      "source": [
        "results = load_results()\n",
        "\n",
        "name = \"baseline_trained\"\n",
        "if name not in results:\n",
        "    cfg = ExpConfig(\n",
        "        name=name,\n",
        "        use_man=False,\n",
        "        use_hnm=False,\n",
        "        unfreeze_last_vision=UNFREEZE_LAST_VISION_DEFAULT,\n",
        "        unfreeze_last_text=UNFREEZE_LAST_TEXT_DEFAULT,\n",
        "        # hnm_warmup_epochs not relevant when use_hnm=False\n",
        "    )\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"Running: {cfg.name} | MAN={cfg.use_man} | HNM={cfg.use_hnm}\")\n",
        "    results[name] = train_one_experiment(cfg)\n",
        "    save_results(results)\n",
        "else:\n",
        "    print(f\"Skipping {name} (already saved)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3t-53gPjinPO",
      "metadata": {
        "id": "3t-53gPjinPO"
      },
      "outputs": [],
      "source": [
        "results = load_results()\n",
        "\n",
        "name = \"MAN_only\"\n",
        "if name not in results:\n",
        "    cfg = ExpConfig(name=name, use_man=True, use_hnm=False)\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"Running: {cfg.name} | MAN={cfg.use_man} | HNM={cfg.use_hnm}\")\n",
        "    results[name] = train_one_experiment(cfg)\n",
        "    save_results(results)\n",
        "else:\n",
        "    print(f\"Skipping {name} (already saved)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5ea7285",
      "metadata": {
        "id": "c5ea7285"
      },
      "outputs": [],
      "source": [
        "results = load_results()\n",
        "\n",
        "name = \"MAN_only(encoder frozen)\"\n",
        "if name not in results:\n",
        "    cfg = ExpConfig(\n",
        "        name=name,\n",
        "        use_man=True,\n",
        "        use_hnm=False,\n",
        "        unfreeze_last_vision=UNFREEZE_LAST_VISION_DEFAULT,\n",
        "        unfreeze_last_text=UNFREEZE_LAST_TEXT_DEFAULT,\n",
        "        man_l2_train=True,  # keep consistent unless you want to ablate this too\n",
        "    )\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"Running: {cfg.name} | MAN={cfg.use_man} | HNM={cfg.use_hnm}\")\n",
        "    results[name] = train_one_experiment(cfg)\n",
        "    save_results(results)\n",
        "else:\n",
        "    print(f\"Skipping {name} (already saved)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506e2d77",
      "metadata": {
        "id": "506e2d77"
      },
      "outputs": [],
      "source": [
        "results = load_results()\n",
        "\n",
        "name = \"HNM_only\"\n",
        "if name not in results:\n",
        "    cfg = ExpConfig(\n",
        "        name=name,\n",
        "        use_man=False,\n",
        "        use_hnm=True,\n",
        "        hnm_k=HNM_K_DEFAULT,\n",
        "        hnm_warmup_epochs=2,\n",
        "        unfreeze_last_vision=UNFREEZE_LAST_VISION_DEFAULT,\n",
        "        unfreeze_last_text=UNFREEZE_LAST_TEXT_DEFAULT,\n",
        "    )\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"Running: {cfg.name} | MAN={cfg.use_man} | HNM={cfg.use_hnm}\")\n",
        "    results[name] = train_one_experiment(cfg)\n",
        "    save_results(results)\n",
        "else:\n",
        "    print(f\"Skipping {name} (already saved)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hMCfnIerir40",
      "metadata": {
        "id": "hMCfnIerir40"
      },
      "outputs": [],
      "source": [
        "results = load_results()\n",
        "\n",
        "name = \"MAN_HNM\"\n",
        "if name not in results:\n",
        "    cfg = ExpConfig(name=name, use_man=True, use_hnm=True, hnm_k=HNM_K_DEFAULT)\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"Running: {cfg.name} | MAN={cfg.use_man} | HNM={cfg.use_hnm}\")\n",
        "    results[name] = train_one_experiment(cfg)\n",
        "    save_results(results)\n",
        "else:\n",
        "    print(f\"Skipping {name} (already saved)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "479c4f98",
      "metadata": {
        "id": "479c4f98"
      },
      "outputs": [],
      "source": [
        "results = load_results()\n",
        "\n",
        "name = \"MAN_HNM\"\n",
        "if name not in results:\n",
        "    cfg = ExpConfig(\n",
        "        name=name,\n",
        "        use_man=True,\n",
        "        use_hnm=True,\n",
        "        hnm_k=HNM_K_DEFAULT,\n",
        "        hnm_warmup_epochs=2,\n",
        "        unfreeze_last_vision=UNFREEZE_LAST_VISION_DEFAULT,\n",
        "        unfreeze_last_text=UNFREEZE_LAST_TEXT_DEFAULT,\n",
        "        man_l2_train=True,\n",
        "    )\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"Running: {cfg.name} | MAN={cfg.use_man} | HNM={cfg.use_hnm}\")\n",
        "    results[name] = train_one_experiment(cfg)\n",
        "    save_results(results)\n",
        "else:\n",
        "    print(f\"Skipping {name} (already saved)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TjdwcgZmiuMX",
      "metadata": {
        "id": "TjdwcgZmiuMX"
      },
      "outputs": [],
      "source": [
        "results = load_results()\n",
        "\n",
        "required = [\"pretrained_baseline\", \"baseline_trained\", \"MAN_only\", \"HNM_only\", \"MAN_HNM\"]\n",
        "missing = [k for k in required if k not in results]\n",
        "\n",
        "if missing:\n",
        "    print(\"Missing experiments:\", missing)\n",
        "else:\n",
        "    print(\"All experiments are present!\")\n",
        "\n",
        "\n",
        "for k in required:\n",
        "    if k in results:\n",
        "        m = results[k][\"metrics\"]\n",
        "        print(f\"\\n{k}: i2t R@1={m['i2t_R@1']:.4f}, R@5={m['i2t_R@5']:.4f}, R@10={m['i2t_R@10']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8648e62b",
      "metadata": {
        "id": "8648e62b"
      },
      "source": [
        "## 8) Plots + deltas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7482568b",
      "metadata": {
        "id": "7482568b"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "for name, res in results.items():\n",
        "    if len(res[\"history\"][\"train_loss\"]) == 0:\n",
        "        continue\n",
        "    plt.plot(range(1, len(res[\"history\"][\"train_loss\"])+1), res[\"history\"][\"train_loss\"], label=f\"{name}-train\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss\"); plt.legend(); plt.show()\n",
        "\n",
        "plt.figure()\n",
        "for name, res in results.items():\n",
        "    if len(res[\"history\"][\"val_loss\"]) == 0:\n",
        "        continue\n",
        "    plt.plot(range(1, len(res[\"history\"][\"val_loss\"])+1), res[\"history\"][\"val_loss\"], label=f\"{name}-val\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Validation Loss\"); plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c68a53",
      "metadata": {
        "id": "69c68a53"
      },
      "outputs": [],
      "source": [
        "labels = list(results.keys())\n",
        "x = np.arange(len(labels))\n",
        "w = 0.25\n",
        "\n",
        "def m(name, key):\n",
        "    return results[name][\"metrics\"][key]\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.bar(x-w, [m(n,\"i2t_R@1\") for n in labels], width=w, label=\"R@1\")\n",
        "plt.bar(x,   [m(n,\"i2t_R@5\") for n in labels], width=w, label=\"R@5\")\n",
        "plt.bar(x+w, [m(n,\"i2t_R@10\") for n in labels], width=w, label=\"R@10\")\n",
        "plt.xticks(x, labels, rotation=20)\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Image → Text Recall@K\")\n",
        "plt.legend(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.bar(x-w, [m(n,\"t2i_R@1\") for n in labels], width=w, label=\"R@1\")\n",
        "plt.bar(x,   [m(n,\"t2i_R@5\") for n in labels], width=w, label=\"R@5\")\n",
        "plt.bar(x+w, [m(n,\"t2i_R@10\") for n in labels], width=w, label=\"R@10\")\n",
        "plt.xticks(x, labels, rotation=20)\n",
        "plt.ylim(0,1)\n",
        "plt.title(\"Text → Image Recall@K\")\n",
        "plt.legend(); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2a9e5c",
      "metadata": {
        "id": "5b2a9e5c"
      },
      "outputs": [],
      "source": [
        "base = results[\"pretrained_baseline\"][\"metrics\"]\n",
        "keys = [\n",
        "    \"i2t_R@1\",\"i2t_R@5\",\"i2t_R@10\",\n",
        "    \"t2i_R@1\",\"t2i_R@5\",\"t2i_R@10\",\n",
        "    \"i2t_mean_rank\",\"i2t_median_rank\",\n",
        "    \"t2i_mean_rank\",\"t2i_median_rank\"\n",
        "]\n",
        "\n",
        "def fmt(x): return f\"{x:.4f}\" if isinstance(x, float) else str(x)\n",
        "\n",
        "print(\"Pretrained baseline:\")\n",
        "for k in keys:\n",
        "    print(f\"  {k}: {fmt(base[k])}\")\n",
        "\n",
        "print(\"\\nΔ vs pretrained baseline:\")\n",
        "for name, res in results.items():\n",
        "    if name == \"pretrained_baseline\":\n",
        "        continue\n",
        "    print(\"\\n\", name)\n",
        "    mtr = res[\"metrics\"]\n",
        "    for k in keys:\n",
        "        d = mtr[k] - base[k]\n",
        "        print(f\"  {k}: {fmt(mtr[k])}  (Δ {d:+.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization & Comparison Block"
      ],
      "metadata": {
        "id": "XPQafWgQTadc"
      },
      "id": "XPQafWgQTadc"
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ALL PLOTS + AUTO-SAVE (Drive + Download ZIP)\n",
        "# Compatible with this notebook's results.json structure\n",
        "# Generates (each as separate plot):\n",
        "# 1) Train loss curves\n",
        "# 2) Val loss curves\n",
        "# 3) Bar charts: i2t R@{1,5,10}\n",
        "# 4) Bar charts: t2i R@{1,5,10}\n",
        "# 5) Bar charts: mean/median rank (i2t + t2i)\n",
        "# 6) Delta vs baseline (baseline_trained if exists else pretrained_baseline)\n",
        "# 7) Extra: recalls table + scatter (last val loss vs i2t R@1)\n",
        "# Saves all plots to /content/siglip_plots_<timestamp>/, copies to Drive, downloads ZIP.\n",
        "# =========================\n",
        "\n",
        "import os, json, time, shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --------------------------\n",
        "# A) Plot saving utilities\n",
        "# --------------------------\n",
        "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "PLOTS_DIR = os.path.join(\"/content\", f\"siglip_plots_{timestamp}\")\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "_plot_counter = 0\n",
        "def save_show(filename_prefix: str):\n",
        "    global _plot_counter\n",
        "    _plot_counter += 1\n",
        "    safe = \"\".join(ch if ch.isalnum() or ch in \"-_.\" else \"_\" for ch in filename_prefix)\n",
        "    path = os.path.join(PLOTS_DIR, f\"{_plot_counter:02d}_{safe}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(\"✅ Saved:\", path)\n",
        "\n",
        "print(\"📁 Saving plots to:\", PLOTS_DIR)\n",
        "\n",
        "def copy_plots_to_drive(drive_subdir=\"siglip_ablation_outputs\"):\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    drive_dir = f\"/content/drive/MyDrive/{drive_subdir}/{timestamp}\"\n",
        "    os.makedirs(drive_dir, exist_ok=True)\n",
        "    dst = os.path.join(drive_dir, os.path.basename(PLOTS_DIR))\n",
        "    shutil.copytree(PLOTS_DIR, dst, dirs_exist_ok=True)\n",
        "    print(\"✅ Copied plots folder to Drive:\", dst)\n",
        "    return dst\n",
        "\n",
        "def zip_and_download_plots():\n",
        "    from google.colab import files\n",
        "    zip_path = shutil.make_archive(PLOTS_DIR, \"zip\", PLOTS_DIR)\n",
        "    print(\"✅ Created zip:\", zip_path)\n",
        "    files.download(zip_path)\n",
        "\n",
        "# --------------------------\n",
        "# B) Locate results.json\n",
        "# --------------------------\n",
        "candidates = []\n",
        "if \"RESULTS_DIR\" in globals():\n",
        "    candidates.append(os.path.join(RESULTS_DIR, \"results.json\"))\n",
        "if \"RUNS_DIR\" in globals():\n",
        "    candidates.append(os.path.join(RUNS_DIR, \"results.json\"))  # fallback\n",
        "candidates += [\n",
        "    \"/content/ablation_results/results.json\",\n",
        "    \"/content/ablation_runs/results.json\",\n",
        "]\n",
        "\n",
        "results_path = next((p for p in candidates if os.path.exists(p)), None)\n",
        "if results_path is None:\n",
        "    raise FileNotFoundError(\"Could not find results.json. Looked in:\\n\" + \"\\n\".join(candidates))\n",
        "\n",
        "with open(results_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "print(\"✅ Loaded:\", results_path)\n",
        "print(\"Experiments:\", list(results.keys()))\n",
        "\n",
        "# --------------------------\n",
        "# C) Flatten results -> DataFrame\n",
        "# --------------------------\n",
        "rows = []\n",
        "for exp_name, res in results.items():\n",
        "    cfg = res.get(\"config\", {}) or {}\n",
        "    metrics = res.get(\"metrics\", {}) or {}\n",
        "    hist = res.get(\"history\", {}) or {}\n",
        "\n",
        "    row = {\"exp\": exp_name}\n",
        "\n",
        "    for k in [\n",
        "        \"use_man\",\"use_hnm\",\"epochs\",\"lr\",\"weight_decay\",\n",
        "        \"unfreeze_last_vision\",\"unfreeze_last_text\",\n",
        "        \"hnm_k\",\"hnm_warmup_epochs\",\"man_l2_train\"\n",
        "    ]:\n",
        "        if k in cfg:\n",
        "            row[k] = cfg[k]\n",
        "\n",
        "    for mk, mv in metrics.items():\n",
        "        row[mk] = mv\n",
        "\n",
        "    row[\"train_loss_list\"] = hist.get(\"train_loss\", []) or []\n",
        "    row[\"val_loss_list\"]   = hist.get(\"val_loss\", []) or []\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "preferred_order = [\"pretrained_baseline\", \"baseline_trained\", \"MAN_only\", \"HNM_only\", \"MAN_HNM\"]\n",
        "df[\"order\"] = df[\"exp\"].apply(lambda x: preferred_order.index(x) if x in preferred_order else 999)\n",
        "df = df.sort_values([\"order\", \"exp\"]).reset_index(drop=True).drop(columns=[\"order\"])\n",
        "\n",
        "display(df)\n",
        "\n",
        "exp_names = df[\"exp\"].tolist()\n",
        "\n",
        "def _as_list(x):\n",
        "    return x if isinstance(x, list) else []\n",
        "\n",
        "def _to_float_series(col):\n",
        "    return pd.to_numeric(df[col], errors=\"coerce\") if col in df.columns else None\n",
        "\n",
        "# --------------------------\n",
        "# D) Loss curves\n",
        "# --------------------------\n",
        "plt.figure()\n",
        "for _, r in df.iterrows():\n",
        "    tr = _as_list(r[\"train_loss_list\"])\n",
        "    if len(tr) == 0:\n",
        "        continue\n",
        "    plt.plot(range(1, len(tr) + 1), tr, marker=\"o\", label=r[\"exp\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Train Loss\")\n",
        "plt.title(\"Train Loss vs Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "save_show(\"loss_train_vs_epoch\")\n",
        "\n",
        "plt.figure()\n",
        "for _, r in df.iterrows():\n",
        "    vl = _as_list(r[\"val_loss_list\"])\n",
        "    if len(vl) == 0:\n",
        "        continue\n",
        "    plt.plot(range(1, len(vl) + 1), vl, marker=\"o\", label=r[\"exp\"])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Val Loss\")\n",
        "plt.title(\"Validation Loss vs Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "save_show(\"loss_val_vs_epoch\")\n",
        "\n",
        "# --------------------------\n",
        "# E) Recall@K bar charts\n",
        "# --------------------------\n",
        "def bar_metric(col, title, ylabel, fname):\n",
        "    if col not in df.columns:\n",
        "        print(f\"Missing column: {col} (skipping)\")\n",
        "        return\n",
        "    vals = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    plt.figure()\n",
        "    plt.bar(exp_names, vals)\n",
        "    plt.xlabel(\"Experiment\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=20, ha=\"right\")\n",
        "    plt.grid(True, axis=\"y\")\n",
        "    save_show(fname)\n",
        "\n",
        "for k in [1, 5, 10]:\n",
        "    bar_metric(f\"i2t_R@{k}\", f\"Image → Text Recall@{k}\", f\"i2t R@{k}\", f\"bars_i2t_R@{k}\")\n",
        "for k in [1, 5, 10]:\n",
        "    bar_metric(f\"t2i_R@{k}\", f\"Text → Image Recall@{k}\", f\"t2i R@{k}\", f\"bars_t2i_R@{k}\")\n",
        "\n",
        "# --------------------------\n",
        "# F) Rank bars (lower is better)\n",
        "# --------------------------\n",
        "rank_cols = [\n",
        "    (\"i2t_mean_rank\",   \"Image → Text Mean Rank (lower is better)\"),\n",
        "    (\"t2i_mean_rank\",   \"Text → Image Mean Rank (lower is better)\"),\n",
        "    (\"i2t_median_rank\", \"Image → Text Median Rank (lower is better)\"),\n",
        "    (\"t2i_median_rank\", \"Text → Image Median Rank (lower is better)\"),\n",
        "]\n",
        "for col, title in rank_cols:\n",
        "    if col not in df.columns:\n",
        "        continue\n",
        "    vals = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    plt.figure()\n",
        "    plt.bar(exp_names, vals)\n",
        "    plt.xlabel(\"Experiment\")\n",
        "    plt.ylabel(col)\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=20, ha=\"right\")\n",
        "    plt.grid(True, axis=\"y\")\n",
        "    save_show(f\"bars_{col}\")\n",
        "\n",
        "# --------------------------\n",
        "# G) Delta vs baseline (fallback)\n",
        "# --------------------------\n",
        "baseline_name = \"baseline_trained\" if \"baseline_trained\" in exp_names else (\"pretrained_baseline\" if \"pretrained_baseline\" in exp_names else None)\n",
        "if baseline_name is None:\n",
        "    print(\"No baseline found for delta plots.\")\n",
        "else:\n",
        "    base_row = df[df[\"exp\"] == baseline_name].iloc[0]\n",
        "    print(\"Δ plots baseline:\", baseline_name)\n",
        "\n",
        "    for col in [\"i2t_R@1\",\"t2i_R@1\",\"i2t_R@5\",\"t2i_R@5\",\"i2t_R@10\",\"t2i_R@10\"]:\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "        base_val = float(pd.to_numeric(base_row[col], errors=\"coerce\"))\n",
        "        vals = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "        deltas = vals - base_val\n",
        "\n",
        "        plt.figure()\n",
        "        plt.bar(exp_names, deltas)\n",
        "        plt.axhline(0.0)\n",
        "        plt.xlabel(\"Experiment\")\n",
        "        plt.ylabel(f\"Δ {col} vs {baseline_name}\")\n",
        "        plt.title(f\"Delta {col} relative to {baseline_name}\")\n",
        "        plt.xticks(rotation=20, ha=\"right\")\n",
        "        plt.grid(True, axis=\"y\")\n",
        "        save_show(f\"delta_{col}_vs_{baseline_name}\")\n",
        "\n",
        "    for col in [\"i2t_mean_rank\",\"t2i_mean_rank\",\"i2t_median_rank\",\"t2i_median_rank\"]:\n",
        "        if col not in df.columns:\n",
        "            continue\n",
        "        base_val = float(pd.to_numeric(base_row[col], errors=\"coerce\"))\n",
        "        vals = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "        deltas = vals - base_val  # negative = improvement (lower is better)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.bar(exp_names, deltas)\n",
        "        plt.axhline(0.0)\n",
        "        plt.xlabel(\"Experiment\")\n",
        "        plt.ylabel(f\"Δ {col} vs {baseline_name}\")\n",
        "        plt.title(f\"Delta {col} relative to {baseline_name} (negative is better)\")\n",
        "        plt.xticks(rotation=20, ha=\"right\")\n",
        "        plt.grid(True, axis=\"y\")\n",
        "        save_show(f\"delta_{col}_vs_{baseline_name}_negative_is_better\")\n",
        "\n",
        "# --------------------------\n",
        "# H) Extra: recalls table\n",
        "# --------------------------\n",
        "rec_cols = [c for c in [\"i2t_R@1\",\"i2t_R@5\",\"i2t_R@10\",\"t2i_R@1\",\"t2i_R@5\",\"t2i_R@10\"] if c in df.columns]\n",
        "if rec_cols:\n",
        "    rec_df = df[[\"exp\"] + rec_cols].copy()\n",
        "    display(rec_df)\n",
        "\n",
        "# --------------------------\n",
        "# I) Extra: Scatter last val loss vs i2t R@1\n",
        "# --------------------------\n",
        "if \"i2t_R@1\" in df.columns:\n",
        "    last_val_loss = df[\"val_loss_list\"].apply(lambda x: _as_list(x)[-1] if len(_as_list(x)) > 0 else np.nan)\n",
        "    r1 = pd.to_numeric(df[\"i2t_R@1\"], errors=\"coerce\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.scatter(last_val_loss, r1)\n",
        "    for i, name in enumerate(exp_names):\n",
        "        x = last_val_loss.iloc[i]\n",
        "        y = r1.iloc[i]\n",
        "        if not (np.isnan(x) or np.isnan(y)):\n",
        "            plt.text(x, y, name, fontsize=8)\n",
        "    plt.xlabel(\"Last Val Loss\")\n",
        "    plt.ylabel(\"i2t R@1\")\n",
        "    plt.title(\"Val Loss (last epoch) vs i2t R@1\")\n",
        "    plt.grid(True)\n",
        "    save_show(\"scatter_last_val_loss_vs_i2t_R@1\")\n",
        "\n",
        "# --------------------------\n",
        "# J) Save to Drive + Download ZIP\n",
        "# --------------------------\n",
        "drive_folder = copy_plots_to_drive(\"siglip_ablation_outputs\")\n",
        "zip_and_download_plots()\n",
        "\n",
        "print(\" Done.\")\n",
        "print(\"Local plots:\", PLOTS_DIR)\n",
        "print(\"Drive folder:\", drive_folder)\n"
      ],
      "metadata": {
        "id": "_MfDHArrR9vH"
      },
      "id": "_MfDHArrR9vH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _list_to_row(loss_list, max_len):\n",
        "    loss_list = loss_list if isinstance(loss_list, list) else []\n",
        "    out = [np.nan] * max_len\n",
        "    for i in range(min(len(loss_list), max_len)):\n",
        "        out[i] = float(loss_list[i])\n",
        "    return out\n",
        "\n",
        "# Find max epochs across experiments\n",
        "max_ep = int(df[\"train_loss_list\"].apply(lambda x: len(x) if isinstance(x, list) else 0).max())\n",
        "max_ep = max(max_ep, int(df[\"val_loss_list\"].apply(lambda x: len(x) if isinstance(x, list) else 0).max()))\n",
        "\n",
        "if max_ep == 0:\n",
        "    raise ValueError(\"No loss lists found in df (train_loss_list / val_loss_list are empty).\")\n",
        "\n",
        "epochs = [f\"E{e}\" for e in range(1, max_ep + 1)]\n",
        "\n",
        "train_mat = np.vstack(df[\"train_loss_list\"].apply(lambda x: _list_to_row(x, max_ep)).to_numpy())\n",
        "val_mat   = np.vstack(df[\"val_loss_list\"].apply(lambda x: _list_to_row(x, max_ep)).to_numpy())\n",
        "\n",
        "# --- Train heatmap ---\n",
        "plt.figure()\n",
        "plt.imshow(train_mat, aspect=\"auto\")\n",
        "plt.colorbar(label=\"Train Loss\")\n",
        "plt.yticks(range(len(df)), df[\"exp\"].tolist())\n",
        "plt.xticks(range(max_ep), epochs)\n",
        "plt.title(\"Train Loss Heatmap (Experiment × Epoch)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Experiment\")\n",
        "plt.show()\n",
        "\n",
        "# --- Val heatmap ---\n",
        "plt.figure()\n",
        "plt.imshow(val_mat, aspect=\"auto\")\n",
        "plt.colorbar(label=\"Val Loss\")\n",
        "plt.yticks(range(len(df)), df[\"exp\"].tolist())\n",
        "plt.xticks(range(max_ep), epochs)\n",
        "plt.title(\"Validation Loss Heatmap (Experiment × Epoch)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Experiment\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sle35vbgqyIE"
      },
      "id": "sle35vbgqyIE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
        "\n",
        "def plot_loss_3d(df, col_list, title):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "    for y, (_, r) in enumerate(df.iterrows()):\n",
        "        losses = r[col_list] if isinstance(r[col_list], list) else []\n",
        "        if len(losses) == 0:\n",
        "            continue\n",
        "        x = np.arange(1, len(losses) + 1)\n",
        "        z = np.array(losses, dtype=float)\n",
        "        yy = np.full_like(x, y)\n",
        "        ax.plot(x, yy, z, marker=\"o\", label=r[\"exp\"])\n",
        "\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Experiment\")\n",
        "    ax.set_zlabel(\"Loss\")\n",
        "    ax.set_yticks(range(len(df)))\n",
        "    ax.set_yticklabels(df[\"exp\"].tolist())\n",
        "    ax.set_title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_3d(df, \"train_loss_list\", \"3D Train Loss (Epoch × Experiment × Loss)\")\n",
        "plot_loss_3d(df, \"val_loss_list\", \"3D Val Loss (Epoch × Experiment × Loss)\")\n"
      ],
      "metadata": {
        "id": "OPBpZDFmrB8D"
      },
      "id": "OPBpZDFmrB8D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTIONAL: Qualitative retrieval examples"
      ],
      "metadata": {
        "id": "iPe9rhMyTqzK"
      },
      "id": "iPe9rhMyTqzK"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Qualitative retrieval: compare two experiments on a small val subset =====\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import SiglipModel, SiglipProcessor\n",
        "\n",
        "# --- Config ---\n",
        "QUAL_ENABLE = True\n",
        "QUAL_EXP_A = \"pretrained_baseline\"\n",
        "QUAL_EXP_B = \"MAN_HNM\"\n",
        "QUAL_NUM_EXAMPLES = 7\n",
        "QUAL_TOPK = 5\n",
        "QUAL_MAX_VAL_SAMPLES = 200  # keep small for speed\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if QUAL_ENABLE:\n",
        "    try:\n",
        "        # Use the same processor as training\n",
        "        _proc = SiglipProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        # ---- Helper: locate checkpoint dir ----\n",
        "        def _get_exp_dir(exp_name: str) -> str:\n",
        "            # In this notebook checkpoints were saved under RESULTS_DIR/<exp_name>\n",
        "            p1 = os.path.join(RESULTS_DIR, exp_name)\n",
        "            p2 = os.path.join(RUNS_DIR, exp_name) if \"RUNS_DIR\" in globals() else None\n",
        "            if os.path.exists(p1):\n",
        "                return p1\n",
        "            if p2 is not None and os.path.exists(p2):\n",
        "                return p2\n",
        "            raise FileNotFoundError(\n",
        "                f\"Missing checkpoint dir for {exp_name}. Tried: {p1}\" + (f\", {p2}\" if p2 else \"\")\n",
        "            )\n",
        "\n",
        "        # ---- Helper: load MAN head if exists (state_dict only; infer embed_dim) ----\n",
        "        # Requires MANHead class to be defined in your notebook.\n",
        "        def _load_man_if_exists(exp_dir: str):\n",
        "            man_path = os.path.join(exp_dir, \"man_head.pt\")\n",
        "            if not os.path.exists(man_path):\n",
        "                return None\n",
        "\n",
        "            sd = torch.load(man_path, map_location=\"cpu\")  # plain state_dict\n",
        "\n",
        "            # Infer embed_dim from LayerNorm weights\n",
        "            if \"ln_img.weight\" in sd:\n",
        "                embed_dim = int(sd[\"ln_img.weight\"].numel())\n",
        "            elif \"ln_txt.weight\" in sd:\n",
        "                embed_dim = int(sd[\"ln_txt.weight\"].numel())\n",
        "            else:\n",
        "                raise KeyError(\n",
        "                    f\"Could not infer embed_dim from man_head state_dict keys. \"\n",
        "                    f\"Example keys: {list(sd.keys())[:12]}\"\n",
        "                )\n",
        "\n",
        "            man = MANHead(embed_dim)  # matches your MANHead(embed_dim) definition\n",
        "            man.load_state_dict(sd)\n",
        "            man.eval()\n",
        "            return man\n",
        "\n",
        "        # ---- Helper: subset a PyTorch Dataset by indices ----\n",
        "        class SubsetDataset(Dataset):\n",
        "            def __init__(self, base_ds, indices):\n",
        "                self.base_ds = base_ds\n",
        "                self.indices = list(indices)\n",
        "\n",
        "            def __len__(self):\n",
        "                return len(self.indices)\n",
        "\n",
        "            def __getitem__(self, i):\n",
        "                return self.base_ds[self.indices[i]]\n",
        "\n",
        "        def _subset_val_dataset_pytorch(dataset, n: int, seed: int = 0):\n",
        "            n = min(n, len(dataset))\n",
        "            rng = np.random.default_rng(seed)\n",
        "            idx = rng.choice(len(dataset), size=n, replace=False)\n",
        "            return SubsetDataset(dataset, idx), idx\n",
        "\n",
        "        # ---- Collate that also keeps raw captions for printing ----\n",
        "        def collate_with_texts(batch):\n",
        "            images, texts = zip(*batch)  # LocalImageTextDataset returns (PIL.Image, text)\n",
        "            batch_t = _proc(\n",
        "                images=list(images),\n",
        "                text=list(texts),\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=MAX_LEN,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            batch_t[\"raw_texts\"] = list(texts)\n",
        "            return batch_t\n",
        "\n",
        "        def _load_exp_model(exp_name: str):\n",
        "            exp_dir = _get_exp_dir(exp_name)\n",
        "            model = SiglipModel.from_pretrained(exp_dir).to(DEVICE)\n",
        "            model.eval()\n",
        "\n",
        "            man = _load_man_if_exists(exp_dir)\n",
        "            if man is not None:\n",
        "                man = man.to(DEVICE)\n",
        "                man.eval()\n",
        "\n",
        "            return model, man, exp_dir\n",
        "\n",
        "        @torch.no_grad()\n",
        "        def _compute_embeds_for_dataset(model, man, dataset):\n",
        "            dl = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                shuffle=False,\n",
        "                num_workers=0,\n",
        "                collate_fn=collate_with_texts,\n",
        "            )\n",
        "\n",
        "            all_i, all_t, all_caps = [], [], []\n",
        "            for batch in dl:\n",
        "                all_caps.extend(batch[\"raw_texts\"])\n",
        "\n",
        "                batch_t = {k: v.to(DEVICE) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
        "                out = model(**batch_t)\n",
        "\n",
        "                zi = out.image_embeds\n",
        "                zt = out.text_embeds\n",
        "\n",
        "                if man is not None:\n",
        "                    zi, zt = man(zi, zt, l2_normalize=True)\n",
        "                else:\n",
        "                    zi = F.normalize(zi, dim=-1)\n",
        "                    zt = F.normalize(zt, dim=-1)\n",
        "\n",
        "                all_i.append(zi.float().cpu())\n",
        "                all_t.append(zt.float().cpu())\n",
        "\n",
        "            img_emb = torch.cat(all_i, dim=0)\n",
        "            txt_emb = torch.cat(all_t, dim=0)\n",
        "            return img_emb, txt_emb, all_caps\n",
        "\n",
        "        def _topk_texts_for_image(sim_row: torch.Tensor, captions, k: int):\n",
        "            vals, idx = torch.topk(sim_row, k=min(k, sim_row.numel()))\n",
        "            out = []\n",
        "            for v, i in zip(vals, idx):\n",
        "                ii = int(i)\n",
        "                out.append((ii, float(v), captions[ii]))\n",
        "            return out\n",
        "\n",
        "        # ---- Build small val subset (PyTorch-safe) ----\n",
        "        val_small, _ = _subset_val_dataset_pytorch(val_ds, QUAL_MAX_VAL_SAMPLES, seed=SEED)\n",
        "\n",
        "        # ---- Load two experiments ----\n",
        "        print(f\"\\n[QUAL] Loading models: {QUAL_EXP_A} vs {QUAL_EXP_B}\")\n",
        "        model_a, man_a, dir_a = _load_exp_model(QUAL_EXP_A)\n",
        "        model_b, man_b, dir_b = _load_exp_model(QUAL_EXP_B)\n",
        "        print(\"[QUAL] A dir:\", dir_a)\n",
        "        print(\"[QUAL] B dir:\", dir_b)\n",
        "\n",
        "        # ---- Compute embeddings on the same val subset ----\n",
        "        print(\"[QUAL] Computing embeddings on val subset...\")\n",
        "        img_a, txt_a, caps = _compute_embeds_for_dataset(model_a, man_a, val_small)\n",
        "        img_b, txt_b, _    = _compute_embeds_for_dataset(model_b, man_b, val_small)\n",
        "\n",
        "        sim_a = img_a @ txt_a.T\n",
        "        sim_b = img_b @ txt_b.T\n",
        "\n",
        "        # ---- Pick random examples from subset ----\n",
        "        rng = np.random.default_rng(SEED)\n",
        "        ex_ids = rng.choice(sim_a.shape[0], size=min(QUAL_NUM_EXAMPLES, sim_a.shape[0]), replace=False)\n",
        "\n",
        "        for ex in ex_ids:\n",
        "            pil_img, gt_caption = val_small[int(ex)]\n",
        "\n",
        "            top_a = _topk_texts_for_image(sim_a[int(ex)], caps, QUAL_TOPK)\n",
        "            top_b = _topk_texts_for_image(sim_b[int(ex)], caps, QUAL_TOPK)\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(pil_img)\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"Subset idx={int(ex)} | GT: {gt_caption[:120]}\")\n",
        "            plt.show()\n",
        "\n",
        "            print(f\"\\n--- {QUAL_EXP_A} Top-{QUAL_TOPK} ---\")\n",
        "            for rank, (idx, score, cap) in enumerate(top_a, start=1):\n",
        "                marker = \"✅\" if idx == int(ex) else \"  \"\n",
        "                print(f\"{marker} {rank:>2}. (idx={idx}) score={score:.4f} | {cap}\")\n",
        "\n",
        "            print(f\"\\n--- {QUAL_EXP_B} Top-{QUAL_TOPK} ---\")\n",
        "            for rank, (idx, score, cap) in enumerate(top_b, start=1):\n",
        "                marker = \"✅\" if idx == int(ex) else \"  \"\n",
        "                print(f\"{marker} {rank:>2}. (idx={idx}) score={score:.4f} | {cap}\")\n",
        "\n",
        "        print(\"\\n[QUAL] Done.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n[QUAL] Skipped qualitative section due to error:\")\n",
        "        print(type(e).__name__, \":\", str(e))\n",
        "        print(\"Tip: ensure experiments exist under RESULTS_DIR/<exp_name>/ and that MANHead is defined.\")\n"
      ],
      "metadata": {
        "id": "JQe6D6LhTmhr"
      },
      "id": "JQe6D6LhTmhr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Find and display a \"baseline fails, MAN_HNM succeeds\" example =====\n",
        "FAIL_TOPK = QUAL_TOPK\n",
        "\n",
        "def _topk_indices(sim_row: torch.Tensor, k: int):\n",
        "    _, idx = torch.topk(sim_row, k=min(k, sim_row.numel()))\n",
        "    return set(int(i) for i in idx)\n",
        "\n",
        "# Scan all subset examples\n",
        "candidates = []\n",
        "for i in range(sim_a.shape[0]):\n",
        "    top_a = _topk_indices(sim_a[i], FAIL_TOPK)\n",
        "    top_b = _topk_indices(sim_b[i], FAIL_TOPK)\n",
        "\n",
        "    a_hits = (i in top_a)\n",
        "    b_hits = (i in top_b)\n",
        "\n",
        "    if (not a_hits) and b_hits:\n",
        "        # optional: store \"how bad baseline is\" vs \"how good B is\"\n",
        "        # rank proxy: use similarity gap between GT and best\n",
        "        gt_a = float(sim_a[i, i])\n",
        "        gt_b = float(sim_b[i, i])\n",
        "        candidates.append((i, gt_a, gt_b))\n",
        "\n",
        "print(f\"Found {len(candidates)} candidates where baseline misses Top-{FAIL_TOPK} but MAN_HNM hits.\")\n",
        "\n",
        "if len(candidates) == 0:\n",
        "    print(\"No such examples in this subset. Try increasing QUAL_MAX_VAL_SAMPLES (e.g., 800) or using K=1/3.\")\n",
        "else:\n",
        "    # pick the strongest improvement: highest (gt_b - gt_a)\n",
        "    candidates.sort(key=lambda x: (x[2] - x[1]), reverse=True)\n",
        "    ex, gt_a, gt_b = candidates[0]\n",
        "    print(f\"Showing example idx={ex} | baseline GT sim={gt_a:.4f} | MAN_HNM GT sim={gt_b:.4f}\")\n",
        "\n",
        "    # display the image + GT caption\n",
        "    pil_img, gt_caption = val_small[int(ex)]\n",
        "    plt.figure()\n",
        "    plt.imshow(pil_img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Baseline fails / MAN_HNM succeeds | idx={ex}\\nGT: {gt_caption[:120]}\")\n",
        "    plt.show()\n",
        "\n",
        "    # print Top-K lists for both\n",
        "    top_a_list = _topk_texts_for_image(sim_a[int(ex)], caps, FAIL_TOPK)\n",
        "    top_b_list = _topk_texts_for_image(sim_b[int(ex)], caps, FAIL_TOPK)\n",
        "\n",
        "    print(f\"\\n--- {QUAL_EXP_A} Top-{FAIL_TOPK} ---\")\n",
        "    for rank, (idx, score, cap) in enumerate(top_a_list, start=1):\n",
        "        marker = \"✅\" if idx == int(ex) else \"  \"\n",
        "        print(f\"{marker} {rank:>2}. (idx={idx}) score={score:.4f} | {cap}\")\n",
        "\n",
        "    print(f\"\\n--- {QUAL_EXP_B} Top-{FAIL_TOPK} ---\")\n",
        "    for rank, (idx, score, cap) in enumerate(top_b_list, start=1):\n",
        "        marker = \"✅\" if idx == int(ex) else \"  \"\n",
        "        print(f\"{marker} {rank:>2}. (idx={idx}) score={score:.4f} | {cap}\")\n"
      ],
      "metadata": {
        "id": "5XZEqilInwDF"
      },
      "id": "5XZEqilInwDF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Show multiple cases: baseline fails, MAN_HNM succeeds =====\n",
        "FAIL_TOPK = QUAL_TOPK\n",
        "NUM_CASES_TO_SHOW = 3  # <- כמה דוגמאות להציג\n",
        "\n",
        "def _topk_indices(sim_row: torch.Tensor, k: int):\n",
        "    _, idx = torch.topk(sim_row, k=min(k, sim_row.numel()))\n",
        "    return set(int(i) for i in idx)\n",
        "\n",
        "# Collect candidates\n",
        "candidates = []\n",
        "for i in range(sim_a.shape[0]):\n",
        "    top_a = _topk_indices(sim_a[i], FAIL_TOPK)\n",
        "    top_b = _topk_indices(sim_b[i], FAIL_TOPK)\n",
        "\n",
        "    if (i not in top_a) and (i in top_b):\n",
        "        gt_a = float(sim_a[i, i])\n",
        "        gt_b = float(sim_b[i, i])\n",
        "        candidates.append((i, gt_a, gt_b, gt_b - gt_a))\n",
        "\n",
        "print(f\"Found {len(candidates)} candidates where baseline misses Top-{FAIL_TOPK} but MAN_HNM hits.\")\n",
        "\n",
        "if len(candidates) == 0:\n",
        "    print(\"No such examples in this subset. Try increasing QUAL_MAX_VAL_SAMPLES or using smaller K.\")\n",
        "else:\n",
        "    # sort by biggest improvement\n",
        "    candidates.sort(key=lambda x: x[3], reverse=True)\n",
        "\n",
        "    num_show = min(NUM_CASES_TO_SHOW, len(candidates))\n",
        "    print(f\"Showing {num_show} strongest cases.\\n\")\n",
        "\n",
        "    for j in range(num_show):\n",
        "        ex, gt_a, gt_b, imp = candidates[j]\n",
        "\n",
        "        print(\"=\" * 90)\n",
        "        print(f\"Example #{j+1} | idx={ex} | baseline sim={gt_a:.4f} | MAN_HNM sim={gt_b:.4f} | Δ={imp:.4f}\")\n",
        "\n",
        "        pil_img, gt_caption = val_small[int(ex)]\n",
        "        plt.figure()\n",
        "        plt.imshow(pil_img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Baseline fails / MAN_HNM succeeds\\nGT: {gt_caption[:120]}\")\n",
        "        plt.show()\n",
        "\n",
        "        top_a_list = _topk_texts_for_image(sim_a[int(ex)], caps, FAIL_TOPK)\n",
        "        top_b_list = _topk_texts_for_image(sim_b[int(ex)], caps, FAIL_TOPK)\n",
        "\n",
        "        print(f\"\\n--- {QUAL_EXP_A} Top-{FAIL_TOPK} ---\")\n",
        "        for rank, (idx, score, cap) in enumerate(top_a_list, start=1):\n",
        "            marker = \"✅\" if idx == int(ex) else \"  \"\n",
        "            print(f\"{marker} {rank:>2}. (idx={idx}) score={score:.4f} | {cap}\")\n",
        "\n",
        "        print(f\"\\n--- {QUAL_EXP_B} Top-{FAIL_TOPK} ---\")\n",
        "        for rank, (idx, score, cap) in enumerate(top_b_list, start=1):\n",
        "            marker = \"✅\" if idx == int(ex) else \"  \"\n",
        "            print(f\"{marker} {rank:>2}. (idx={idx}) score={score:.4f} | {cap}\")\n"
      ],
      "metadata": {
        "id": "lSVLMgAMoPe6"
      },
      "id": "lSVLMgAMoPe6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Results"
      ],
      "metadata": {
        "id": "BgkwvZnZSWgV"
      },
      "id": "BgkwvZnZSWgV"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, time\n",
        "from google.colab import drive\n",
        "\n",
        "# 1) Mount Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# 2)\n",
        "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "DRIVE_OUT_DIR = f\"/content/drive/MyDrive/{timestamp}\"\n",
        "os.makedirs(DRIVE_OUT_DIR, exist_ok=True)\n",
        "\n",
        "# 3)\n",
        "RESULTS_PATH = os.path.join(RESULTS_DIR, \"results.json\")\n",
        "if os.path.exists(RESULTS_PATH):\n",
        "    shutil.copy2(RESULTS_PATH, os.path.join(DRIVE_OUT_DIR, \"results.json\"))\n",
        "    print(\"Copied results.json to:\", DRIVE_OUT_DIR)\n",
        "else:\n",
        "    print(\"results.json not found at:\", RESULTS_PATH)\n",
        "\n",
        "#\n",
        "if os.path.exists(RUNS_DIR):\n",
        "    shutil.copytree(RUNS_DIR, os.path.join(DRIVE_OUT_DIR, \"ablation_runs\"), dirs_exist_ok=True)\n",
        "    print(\"Copied RUNS_DIR to:\", os.path.join(DRIVE_OUT_DIR, \"ablation_runs\"))\n",
        "else:\n",
        "    print(\"RUNS_DIR not found:\", RUNS_DIR)\n"
      ],
      "metadata": {
        "id": "SrsIYit5SWQm"
      },
      "id": "SrsIYit5SWQm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "RESULTS_PATH = os.path.join(RESULTS_DIR, \"results.json\")\n",
        "if os.path.exists(RESULTS_PATH):\n",
        "    files.download(RESULTS_PATH)\n",
        "else:\n",
        "    print(\"results.json not found:\", RESULTS_PATH)\n"
      ],
      "metadata": {
        "id": "XYKJ5GFLSlv7"
      },
      "id": "XYKJ5GFLSlv7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}